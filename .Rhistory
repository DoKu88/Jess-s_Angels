library("SuperLearner")
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
head(train[, 5])
library("SuperLearner")
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
# Set the seed
set.seed(150)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
# Review performance of each algorithm and ensemble weights.
result
library("SuperLearner")
# Set the seed
set.seed(150)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
# Review performance of each algorithm and ensemble weights.
result
library("SuperLearner")
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
# Set the seed
set.seed(150)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
# Review performance of each algorithm and ensemble weights.
result
library("SuperLearner")
# Set the seed
set.seed(150)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
# Review performance of each algorithm and ensemble weights.
result
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
library("SuperLearner")
# Set the seed
set.seed(150)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
# Review performance of each algorithm and ensemble weights.
result
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
library("SuperLearner")
# Set the seed
set.seed(150)
#sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
#           "SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
sl_lib = c("SL.xgboost", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
# Review performance of each algorithm and ensemble weights.
result
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
library("SuperLearner")
# Set the seed
set.seed(150)
#sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
#           "SL.bartMachine", "SL.KernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
sl_lib = c("SL.xgboost", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = y, X = x, SL.library = sl_lib)
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
require(SuperLearner)
#require(gam)
#require(caret)
require(randomForest)
require(glmnet)
#require(RCurl)
#require(MASS)
#require(tmle)
#require(ggplot2)
#require(gbm)
# Set the seed
set.seed(150)
sl_lib = c("SL.xgboost", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
system.time({
# This will take about 3x as long as the previous SuperLearner.
cv_sl = CV.SuperLearner(Y = y, X = x, V = 3,
SL.library = c("SL.mean", "SL.glmnet"))
})
summary(cv_sl)
# Review performance of each algorithm and ensemble weights.
result
# Review performance of each algorithm and ensemble weights.
cv_sl
sl_lib = c("SL.mean", "SL.xgboost", "SL.glmnet")
sl = SuperLearner(Y = y, X = x, SL.library = sl_lib)
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
require(SuperLearner)
#require(gam)
#require(caret)
require(randomForest)
require(glmnet)
require(xgboost)
#require(RCurl)
#require(MASS)
#require(tmle)
#require(ggplot2)
#require(gbm)
# Use 2 of those cores for parallel SuperLearner.
# Replace "2" with "num_cores" (without quotes) to use all cores.
options(mc.cores = 2)
# Check how many parallel workers we are using (on macOS/Linux).
getOption("mc.cores")
# We need to set a different type of seed that works across cores.
# Otherwise the other cores will go rogue and we won't get repeatable results.
# This version is for the "multicore" parallel system in R.
set.seed(1, "L'Ecuyer-CMRG")
#sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
#           "SL.bartMachine", "SL.KernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
sl_lib = c("SL.mean", "SL.xgboost", "SL.glmnet")
sl = SuperLearner(Y = y, X = x, SL.library = sl_lib)
train = read.csv(file="data/mu_probe.csv", header=TRUE, sep=",")
test = read.csv(file="data/mu_val.csv", header=TRUE, sep=",")
head(train)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
RMSE = function(true, pred) {
res =  sqrt(sum((true - pred)**2) / length(true))
return(res)
}
require(SuperLearner)
#require(gam)
#require(caret)
require(randomForest)
require(glmnet)
require(xgboost)
#require(kernelKnn)
#require(RCurl)
#require(MASS)
#require(tmle)
#require(ggplot2)
#require(gbm)
# Use 2 of those cores for parallel SuperLearner.
# Replace "2" with "num_cores" (without quotes) to use all cores.
options(mc.cores = 1)
# Check how many parallel workers we are using (on macOS/Linux).
getOption("mc.cores")
# We need to set a different type of seed that works across cores.
# Otherwise the other cores will go rogue and we won't get repeatable results.
# This version is for the "multicore" parallel system in R.
set.seed(1, "L'Ecuyer-CMRG")
#sl_lib = c("SL.xgboost", "SL.randomForest", "SL.nnet", "SL.ksvm",
#           "SL.bartMachine", "SL.KernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# xgboost > mean and glmnet
sl_lib = c("SL.mean", "randomForest", "xgboost")
system.time({
sl = SuperLearner(Y = y, X = x, SL.library = sl_lib)
})
sl
pred = predict(sl, xtest, onlySL = T)
realPred = as.vector(pred$pred)
ytest = as.vector(ytest)
RMSE(ytest, realPred)
sl_lib = c("SL.mean", "xgboost")
system.time({
sl = SuperLearner(Y = y, X = x, SL.library = sl_lib)
})
sl
pred = predict(sl, xtest, onlySL = T)
realPred = as.vector(pred$pred)
ytest = as.vector(ytest)
RMSE(ytest, realPred)
y = train[, 5]
ytest = test[, 5]
x = data.frame(train[, 1:4])
xtest = data.frame(test[, 1:4])
sl_lib = c("SL.mean", "xgboost")
system.time({
sl = SuperLearner(Y = y, X = x, SL.library = sl_lib)
})
sl
pred = predict(sl, xtest, onlySL = T)
realPred = as.vector(pred$pred)
ytest = as.vector(ytest)
RMSE(ytest, realPred)
sl_lib = c("SL.mean", "SL.xgboost")
system.time({
sl = SuperLearner(Y = y, X = x, SL.library = sl_lib)
})
sl
pred = predict(sl, xtest, onlySL = T)
realPred = as.vector(pred$pred)
ytest = as.vector(ytest)
RMSE(ytest, realPred)
############################
# Ec112. Bayesian statistics
# Connor Soohoo
# Problem Set 3 Question 1
############################
# Preliminaries
###################
#clear workspace
rm(list=ls())
#initalize random seed
set.seed(123)
# Set simulation parameters
####################
nObs = 100
xGrid = seq(-1,1, by=0.1)
xPredict = c(-4,-2,0,2,4)
nPredictSamples = 10000
beta0 = 0
beta1 = 1
beta2 = 1
sigma = 1
boundBeta0Grid = 5
sizeBeta0Grid = 0.05
boundBeta1Grid = 2
sizeBeta1Grid = 0.1
boundBeta2Grid = 2
sizeBeta2Grid = 0.1
boundSigmaGrid = 3
sizeSigmaGrid = 0.1
beta0Grid = seq(-boundBeta0Grid,boundBeta0Grid, by = sizeBeta0Grid)
beta1Grid = seq(-boundBeta1Grid,boundBeta1Grid, by = sizeBeta1Grid)
beta2Grid = seq(-boundBeta2Grid,boundBeta2Grid, by = sizeBeta2Grid)
sigmaGrid = seq(0.1,boundSigmaGrid, by = sizeSigmaGrid)
nBeta0Grid = length(beta0Grid)
nBeta1Grid = length(beta1Grid)
nBeta2Grid = length(beta2Grid)
nSigmaGrid = length(sigmaGrid)
# prior
priorSigma = 1 / sigmaGrid^2
# build model related objects
####################
#likelihood
likelihood = function( mRate, bodySize, instar, b0L, b1L, b2L,  sL){
loglike = sum(log(dnorm(y-b0L-b1L*log(bodySize) - b2L*instar, mean = 0, sd = sL)))
like = exp(loglike)
return(like)
}
#compute posterior function
compPost = function(mRate, bodySize, instar){
#initialize local posterior
post = array( rep(-1, nBeta0Grid * nBeta1Grid * nBeta2Grid * nSigmaGrid ),
dim = c(nBeta0Grid, nBeta1Grid, nBeta2Grid, nSigmaGrid ))
# compute posterior
for (nBeta0 in 1:nBeta0Grid) {
b0 = beta0Grid[nBeta0]
for (nBeta1 in 1:nBeta1Grid) {
b1 = beta1Grid[nBeta1]
for (nBeta2 in 1:nBeta2Grid) {
b2 = beta2Grid[nBeta2]
for (nSigma in 1:nSigmaGrid) {
s = sigmaGrid[nSigma]
post[nBeta0,nBeta1,nSigma] = likelihood(mRate, bodySize, instar, b0, b1, b2, s) * priorSigma[nSigma]
}
}
}
}
# normalize posterior
post = post / sum(post)
post = post / (sizeBeta0Grid * sizeBeta1Grid * sizeBeta2Grid * sizeSigmaGrid)
# return
return(post)
}
# FIT MODEL
#######################
#initialize arrays
postFinal = array( rep(-1, nBeta0Grid * nBeta1Grid * nBeta2Grid * nSigmaGrid ),
dim = c(nBeta0Grid, nBeta1Grid, nBeta2Grid, nSigmaGrid ))
#generate data and fit posterior
#x = sample(xGrid, nObs, replace = TRUE )
#y = rnorm(nObs, mean = beta0 + beta1 * x, sd = sigma)
data = read.csv(file="MetabolicRate.csv", header=TRUE, sep=",")
pairs(data)
data.head()
